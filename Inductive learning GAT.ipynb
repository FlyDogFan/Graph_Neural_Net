{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from time import time\n",
    "import os, copy\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import scipy.sparse as sp\n",
    "import pdb\n",
    "import sys\n",
    "sys.setrecursionlimit(99999)\n",
    "\n",
    "\n",
    "def run_dfs(adj, msk, u, ind, nb_nodes):\n",
    "    if msk[u] == -1:\n",
    "        msk[u] = ind\n",
    "        #for v in range(nb_nodes):\n",
    "        for v in adj[u,:].nonzero()[1]:\n",
    "            #if adj[u,v]== 1:\n",
    "            run_dfs(adj, msk, v, ind, nb_nodes)\n",
    "\n",
    "# Use depth-first search to split a graph into subgraphs\n",
    "def dfs_split(adj):\n",
    "    # Assume adj is of shape [nb_nodes, nb_nodes]\n",
    "    nb_nodes = adj.shape[0]\n",
    "    ret = np.full(nb_nodes, -1, dtype=np.int32)\n",
    "\n",
    "    graph_id = 0\n",
    "\n",
    "    for i in range(nb_nodes):\n",
    "        if ret[i] == -1:\n",
    "            run_dfs(adj, ret, i, graph_id, nb_nodes)\n",
    "            graph_id += 1\n",
    "\n",
    "    return ret\n",
    "\n",
    "def test(adj, mapping):\n",
    "    nb_nodes = adj.shape[0]\n",
    "    for i in range(nb_nodes):\n",
    "        #for j in range(nb_nodes):\n",
    "        for j in adj[i, :].nonzero()[1]:\n",
    "            if mapping[i] != mapping[j]:\n",
    "              #  if adj[i,j] == 1:\n",
    "                 return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def find_split(adj, mapping, ds_label):\n",
    "    nb_nodes = adj.shape[0]\n",
    "    dict_splits={}\n",
    "    for i in range(nb_nodes):\n",
    "        #for j in range(nb_nodes):\n",
    "        for j in adj[i, :].nonzero()[1]:\n",
    "            if mapping[i]==0 or mapping[j]==0:\n",
    "                dict_splits[0]=None\n",
    "            elif mapping[i] == mapping[j]:\n",
    "                if ds_label[i]['val'] == ds_label[j]['val'] and ds_label[i]['test'] == ds_label[j]['test']:\n",
    "\n",
    "                    if mapping[i] not in dict_splits.keys():\n",
    "                        if ds_label[i]['val']:\n",
    "                            dict_splits[mapping[i]] = 'val'\n",
    "\n",
    "                        elif ds_label[i]['test']:\n",
    "                            dict_splits[mapping[i]]='test'\n",
    "\n",
    "                        else:\n",
    "                            dict_splits[mapping[i]] = 'train'\n",
    "\n",
    "                    else:\n",
    "                        if ds_label[i]['test']:\n",
    "                            ind_label='test'\n",
    "                        elif ds_label[i]['val']:\n",
    "                            ind_label='val'\n",
    "                        else:\n",
    "                            ind_label='train'\n",
    "                        if dict_splits[mapping[i]]!= ind_label:\n",
    "                            print ('inconsistent labels within a graph exiting!!!')\n",
    "                            return None\n",
    "                else:\n",
    "                    print ('label of both nodes different, exiting!!')\n",
    "                    return None\n",
    "    return dict_splits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_p2p():\n",
    "\n",
    "\n",
    "    print ('Loading G...')\n",
    "    with open('ppi/ppi-G.json') as jsonfile:\n",
    "        g_data = json.load(jsonfile)\n",
    "    print (len(g_data))\n",
    "    G = json_graph.node_link_graph(g_data)\n",
    "\n",
    "    #Extracting adjacency matrix\n",
    "    adj=nx.adjacency_matrix(G)\n",
    "\n",
    "    prev_key=''\n",
    "    for key, value in g_data.items():\n",
    "        if prev_key!=key:\n",
    "            print (key)\n",
    "            prev_key=key\n",
    "\n",
    "    print ('Loading id_map...')\n",
    "    with open('ppi/ppi-id_map.json') as jsonfile:\n",
    "        id_map = json.load(jsonfile)\n",
    "    print (len(id_map))\n",
    "\n",
    "    id_map = {int(k):int(v) for k,v in id_map.items()}\n",
    "    for key, value in id_map.items():\n",
    "        id_map[key]=[value]\n",
    "    print (len(id_map))\n",
    "\n",
    "    print ('Loading features...')\n",
    "    features_=np.load('ppi/ppi-feats.npy')\n",
    "    print (features_.shape)\n",
    "\n",
    "    #standarizing features\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    train_ids = np.array([id_map[n] for n in G.nodes() if not G.node[n]['val'] and not G.node[n]['test']])\n",
    "    train_feats = features_[train_ids[:,0]]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_feats)\n",
    "    features_ = scaler.transform(features_)\n",
    "\n",
    "    features = sp.csr_matrix(features_).tolil()\n",
    "\n",
    "\n",
    "    print ('Loading class_map...')\n",
    "    class_map = {}\n",
    "    with open('ppi/ppi-class_map.json') as jsonfile:\n",
    "        class_map = json.load(jsonfile)\n",
    "    print (len(class_map))\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    #Split graph into sub-graphs\n",
    "    print ('Splitting graph...')\n",
    "    splits=dfs_split(adj)\n",
    "\n",
    "    #Rearrange sub-graph index and append sub-graphs with 1 or 2 nodes to bigger sub-graphs\n",
    "    print ('Re-arranging sub-graph IDs...')\n",
    "    list_splits=splits.tolist()\n",
    "    group_inc=1\n",
    "\n",
    "    for i in range(np.max(list_splits)+1):\n",
    "        if list_splits.count(i)>=3:\n",
    "            splits[np.array(list_splits) == i] =group_inc\n",
    "            group_inc+=1\n",
    "        else:\n",
    "            #splits[np.array(list_splits) == i] = 0\n",
    "            ind_nodes=np.argwhere(np.array(list_splits) == i)\n",
    "            ind_nodes=ind_nodes[:,0].tolist()\n",
    "            split=None\n",
    "            \n",
    "            for ind_node in ind_nodes:\n",
    "                if g_data['nodes'][ind_node]['val']:\n",
    "                    if split is None or split=='val':\n",
    "                        splits[np.array(list_splits) == i] = 21\n",
    "                        split='val'\n",
    "                    else:\n",
    "                        raise ValueError('new node is VAL but previously was {}'.format(split))\n",
    "                elif g_data['nodes'][ind_node]['test']:\n",
    "                    if split is None or split=='test':\n",
    "                        splits[np.array(list_splits) == i] = 23\n",
    "                        split='test'\n",
    "                    else:\n",
    "                        raise ValueError('new node is TEST but previously was {}'.format(split))\n",
    "                else:\n",
    "                    if split is None or split == 'train':\n",
    "                        splits[np.array(list_splits) == i] = 1\n",
    "                        split='train'\n",
    "                    else:\n",
    "                        pdb.set_trace()\n",
    "                        raise ValueError('new node is TRAIN but previously was {}'.format(split))\n",
    "\n",
    "    #counting number of nodes per sub-graph\n",
    "    list_splits=splits.tolist()\n",
    "    nodes_per_graph=[]\n",
    "    for i in range(1,np.max(list_splits) + 1):\n",
    "        nodes_per_graph.append(list_splits.count(i))\n",
    "\n",
    "    #Splitting adj matrix into sub-graphs\n",
    "    subgraph_nodes=np.max(nodes_per_graph)\n",
    "    adj_sub=np.empty((len(nodes_per_graph), subgraph_nodes, subgraph_nodes))\n",
    "    feat_sub = np.empty((len(nodes_per_graph), subgraph_nodes, features.shape[1]))\n",
    "    labels_sub = np.empty((len(nodes_per_graph), subgraph_nodes, 121))\n",
    "\n",
    "    for i in range(1, np.max(list_splits) + 1):\n",
    "        #Creating same size sub-graphs\n",
    "        indexes = np.where(splits == i)[0]\n",
    "        subgraph_=adj[indexes,:][:,indexes]\n",
    "\n",
    "        if subgraph_.shape[0]<subgraph_nodes or subgraph_.shape[1]<subgraph_nodes:\n",
    "            subgraph=np.identity(subgraph_nodes)\n",
    "            feats=np.zeros([subgraph_nodes, features.shape[1]])\n",
    "            labels=np.zeros([subgraph_nodes,121])\n",
    "            #adj\n",
    "            subgraph = sp.csr_matrix(subgraph).tolil()\n",
    "            subgraph[0:subgraph_.shape[0],0:subgraph_.shape[1]]=subgraph_\n",
    "            adj_sub[i-1,:,:]=subgraph.todense()\n",
    "            #feats\n",
    "            feats[0:len(indexes)]=features[indexes,:].todense()\n",
    "            feat_sub[i-1,:,:]=feats\n",
    "            #labels\n",
    "            for j,node in enumerate(indexes):\n",
    "                labels[j,:]=np.array(class_map[str(node)])\n",
    "            labels[indexes.shape[0]:subgraph_nodes,:]=np.zeros([121])\n",
    "            labels_sub[i - 1, :, :] = labels\n",
    "\n",
    "        else:\n",
    "            adj_sub[i - 1, :, :] = subgraph_.todense()\n",
    "            feat_sub[i - 1, :, :]=features[indexes,:].todense()\n",
    "            for j,node in enumerate(indexes):\n",
    "                labels[j,:]=np.array(class_map[str(node)])\n",
    "            labels_sub[i-1, :, :] = labels\n",
    "\n",
    "    # Get relation between id sub-graph and tran,val or test set\n",
    "    dict_splits = find_split(adj, splits, g_data['nodes'])\n",
    "\n",
    "    # Testing if sub graphs are isolated\n",
    "    print ('Are sub-graphs isolated?')\n",
    "    print (test(adj, splits))\n",
    "\n",
    "    #Splitting tensors into train,val and test\n",
    "    train_split=[]\n",
    "    val_split=[]\n",
    "    test_split=[]\n",
    "    for key, value in dict_splits.items():\n",
    "        if dict_splits[key]=='train':\n",
    "            train_split.append(int(key)-1)\n",
    "        elif dict_splits[key] == 'val':\n",
    "            val_split.append(int(key)-1)\n",
    "        elif dict_splits[key] == 'test':\n",
    "            test_split.append(int(key)-1)\n",
    "\n",
    "    train_adj=adj_sub[train_split,:,:]\n",
    "    val_adj=adj_sub[val_split,:,:]\n",
    "    test_adj=adj_sub[test_split,:,:]\n",
    "\n",
    "    train_feat=feat_sub[train_split,:,:]\n",
    "    val_feat = feat_sub[val_split, :, :]\n",
    "    test_feat = feat_sub[test_split, :, :]\n",
    "\n",
    "    train_labels = labels_sub[train_split, :, :]\n",
    "    val_labels = labels_sub[val_split, :, :]\n",
    "    test_labels = labels_sub[test_split, :, :]\n",
    "\n",
    "    train_nodes=np.array(nodes_per_graph[train_split[0]:train_split[-1]+1])\n",
    "    val_nodes = np.array(nodes_per_graph[val_split[0]:val_split[-1]+1])\n",
    "    test_nodes = np.array(nodes_per_graph[test_split[0]:test_split[-1]+1])\n",
    "\n",
    "\n",
    "    #Masks with ones\n",
    "\n",
    "    tr_msk = np.zeros((len(nodes_per_graph[train_split[0]:train_split[-1]+1]), subgraph_nodes))\n",
    "    vl_msk = np.zeros((len(nodes_per_graph[val_split[0]:val_split[-1] + 1]), subgraph_nodes))\n",
    "    ts_msk = np.zeros((len(nodes_per_graph[test_split[0]:test_split[-1]+1]), subgraph_nodes))\n",
    "\n",
    "    for i in range(len(train_nodes)):\n",
    "        for j in range(train_nodes[i]):\n",
    "            tr_msk[i][j] = 1\n",
    "\n",
    "    for i in range(len(val_nodes)):\n",
    "        for j in range(val_nodes[i]):\n",
    "            vl_msk[i][j] = 1\n",
    "\n",
    "    for i in range(len(test_nodes)):\n",
    "        for j in range(test_nodes[i]):\n",
    "            ts_msk[i][j] = 1\n",
    "\n",
    "    return train_adj, val_adj, test_adj, train_feat, val_feat, test_feat, train_labels, val_labels, test_labels, train_nodes, val_nodes, test_nodes, tr_msk, vl_msk, ts_msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return sparse_to_tuple(sp.csr_matrix(features))\n",
    "\n",
    "\n",
    "def preprocess_adj_bias(adj):\n",
    "    num_nodes = adj.shape[0]\n",
    "    adj = adj + sp.eye(num_nodes)  # self-loop\n",
    "    adj[adj > 0.0] = 1.0\n",
    "    return sparse_to_tuple(sp.csr_matrix(adj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_f1(logits, labels, mask):\n",
    "    \"\"\"Accuracy with masking.\"\"\"\n",
    "    predicted = tf.round(tf.nn.sigmoid(logits))\n",
    "\n",
    "    # Use integers to avoid any nasty FP behaviour\n",
    "    predicted = tf.cast(predicted, dtype=tf.int32)\n",
    "    labels = tf.cast(labels, dtype=tf.int32)\n",
    "    mask = tf.cast(mask, dtype=tf.int32)\n",
    "    \n",
    "    # expand the mask so that broadcasting works ([nb_nodes, 1])\n",
    "#     mask = tf.expand_dims(mask, -1)\n",
    "\n",
    "\n",
    "    # Count true positives, true negatives, false positives and false negatives.\n",
    "    tp = tf.count_nonzero(predicted * labels * mask)\n",
    "    tn = tf.count_nonzero((predicted - 1) * (labels - 1) * mask)\n",
    "    fp = tf.count_nonzero(predicted * (labels - 1) * mask)\n",
    "    fn = tf.count_nonzero((predicted - 1) * labels * mask)\n",
    "\n",
    "    # Calculate accuracy, precision, recall and F1 score.\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "    fmeasure = tf.cast(fmeasure, tf.float32)\n",
    "    return fmeasure\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def masked_sigmoid_cross_entropy(logits, labels, mask):\n",
    "    \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "    labels = tf.cast(labels, dtype=tf.float32)\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "    loss=tf.reduce_mean(loss,axis=1)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def sp_attn_head(seq, in_sz, out_sz, adj_mat, activation, nb_nodes, \n",
    "                 in_drop=0.0, coef_drop=0.0, residual=True, sparse=True):\n",
    "    with tf.name_scope('sp_attn'):\n",
    "        if in_drop != 0.0:\n",
    "            seq = tf.nn.dropout(seq, 1.0 - in_drop)\n",
    "        w=glorot([in_sz, out_sz])\n",
    "        b=zeros([out_sz])\n",
    "        if sparse:\n",
    "            seq_fts = dot(seq, w, sparse=True) # activation\n",
    "        else:\n",
    "            seq_fts = dot(seq, w, sparse=False)\n",
    "\n",
    "\n",
    "        # simplest self-attention possible\n",
    "        seq_fts=tf.expand_dims(seq_fts, axis=0)\n",
    "        f_1 = tf.layers.conv1d(seq_fts, 1, 1)\n",
    "        f_2 = tf.layers.conv1d(seq_fts, 1, 1)\n",
    "\n",
    "        f_1 = tf.reshape(f_1, (nb_nodes, 1)) \n",
    "        f_2 = tf.reshape(f_2, (nb_nodes, 1))\n",
    "\n",
    "        f_1 = adj_mat*f_1\n",
    "        f_2 = adj_mat * tf.transpose(f_2, [1,0])\n",
    "\n",
    "        logits = tf.sparse_add(f_1, f_2)\n",
    "        lrelu = tf.SparseTensor(indices=logits.indices, \n",
    "                values=tf.nn.leaky_relu(logits.values), \n",
    "                dense_shape=logits.dense_shape)\n",
    "        coefs = tf.sparse_softmax(lrelu)\n",
    "\n",
    "        if coef_drop != 0.0:\n",
    "            coefs = tf.SparseTensor(indices=coefs.indices,\n",
    "                    values=tf.nn.dropout(coefs.values, 1.0 - coef_drop),\n",
    "                    dense_shape=coefs.dense_shape)\n",
    "        if in_drop != 0.0:\n",
    "            seq_fts = tf.nn.dropout(seq_fts, 1.0 - in_drop)\n",
    "\n",
    "        # As tf.sparse_tensor_dense_matmul expects its arguments to have rank-2,\n",
    "        # here we make an assumption that our input is of batch size 1, and reshape appropriately.\n",
    "        # The method will fail in all other cases!\n",
    "        coefs = tf.sparse_reshape(coefs, [nb_nodes, nb_nodes])\n",
    "        seq_fts = tf.squeeze(seq_fts)\n",
    "        vals = tf.sparse_tensor_dense_matmul(coefs, seq_fts)\n",
    "        ret = vals+b\n",
    "\n",
    "        # residual connection\n",
    "        if residual:\n",
    "            if in_sz != out_sz:\n",
    "                ww=glorot([in_sz, out_sz])\n",
    "                if sparse:\n",
    "                    ret = ret + dot(seq, ww, sparse=True) # activation\n",
    "                else:\n",
    "                    ret = ret + dot(seq, ww, sparse=False)\n",
    "            else:\n",
    "                ret = ret + seq\n",
    "\n",
    "        return activation(ret)  # activation\n",
    "    \n",
    "    \n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res\n",
    "\n",
    "\n",
    "def masked_softmax_cross_entropy(preds, labels, mask):\n",
    "    \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "    loss = -tf.reduce_sum(labels*tf.log(tf.nn.softmax(preds)+1e-7), axis=1)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def masked_accuracy(preds, labels, mask):\n",
    "    \"\"\"Accuracy with masking.\"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    accuracy_all *= mask\n",
    "    return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "\n",
    "def glorot(shape, name=None):\n",
    "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def zeros(shape, name=None):\n",
    "    \"\"\"All zeros.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT_Sparse_Inductive():\n",
    "    def __init__(self, hid_units, n_heads, n_nodes, f_dimension, nb_classes, lr):\n",
    "                    \n",
    "        \n",
    "        \n",
    "        self.X = tf.sparse_placeholder(tf.float32,  name='X')\n",
    "        self.y = tf.placeholder('float32', name='y')\n",
    "        self.mask = tf.placeholder('float32', name='Mask')\n",
    "        self.adj = tf.sparse_placeholder(tf.float32, name='ADJ')\n",
    "\n",
    "        \n",
    "        ########### Input Layer #################\n",
    "        attns = []\n",
    "        for _ in range(n_heads[0]):\n",
    "            attns.append(sp_attn_head(self.X, in_sz=f_dimension , adj_mat=self.adj,out_sz=hid_units[0], activation=tf.nn.elu, nb_nodes=n_nodes,\n",
    "                in_drop=0, coef_drop=0, residual=True, sparse=True))\n",
    "            \n",
    "        h_1 = tf.concat(attns, axis=-1)\n",
    "        \n",
    "        ########### Hidden Layer #################\n",
    "\n",
    "        for i in range(1, len(hid_units)):\n",
    "            h_old = h_1\n",
    "            attns = []\n",
    "            for _ in range(n_heads[i]):\n",
    "                attns.append(sp_attn_head(h_1, in_sz=hid_units[i-1]*n_heads[i-1] ,adj_mat=self.adj,out_sz=hid_units[i], activation=tf.nn.elu, nb_nodes=n_nodes,\n",
    "                    in_drop=0, coef_drop=0, residual=True, sparse=False))\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "        \n",
    "        ########### Output Layer #################\n",
    "        out = []\n",
    "        for _ in range(n_heads[-1]):\n",
    "            out.append(sp_attn_head(h_1, in_sz=hid_units[-2]*n_heads[-2] ,adj_mat=self.adj, out_sz=nb_classes, activation=lambda x: x, nb_nodes=n_nodes,\n",
    "                    in_drop=0, coef_drop=0, residual=True, sparse=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]\n",
    "        self.logits=logits\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.loss(lr)\n",
    "        \n",
    "    def accuracy(self, X, Y, mask, adj):\n",
    "        \"\"\"Get accuracy\"\"\"\n",
    "        return self.sess.run(self.acc,\n",
    "                             feed_dict={self.adj: adj, self.X: X, self.y: Y, self.mask: mask})\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def train(self, batch_xs, batch_ys, mask, adj):\n",
    "        _ = self.sess.run(self.trains, feed_dict={ self.y: batch_ys, self.adj: adj\n",
    "            ,self.X: batch_xs, self.mask: mask})\n",
    "        \n",
    "        \n",
    "\n",
    "    def loss(self, lr):\n",
    "        self.cost = masked_sigmoid_cross_entropy(self.logits, self.y, self.mask) \n",
    "\n",
    "      \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.trains = self.optimizer.minimize(self.cost)\n",
    "        self.acc = micro_f1(self.logits, self.y, self.mask)\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading G...\n",
      "5\n",
      "directed\n",
      "graph\n",
      "nodes\n",
      "links\n",
      "multigraph\n",
      "Loading id_map...\n",
      "56944\n",
      "56944\n",
      "Loading features...\n",
      "(56944, 50)\n",
      "Loading class_map...\n",
      "56944\n",
      "Splitting graph...\n",
      "Re-arranging sub-graph IDs...\n",
      "Are sub-graphs isolated?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "train_adj, val_adj, test_adj, train_feat, val_feat, test_feat, train_labels, val_labels, test_labels, train_nodes, val_nodes, test_nodes, tr_msk, vl_msk, ts_msk =  process_p2p() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature shape:  (3480, 50)\n",
      "adj shape:  (3480, 3480)\n",
      "y_train shape:  (3480, 121)\n",
      "test_set:  5524.0\n",
      "train_set:  44906.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('feature shape: ', train_feat[0].shape)\n",
    "print('adj shape: ', train_adj[0].shape)\n",
    "print('y_train shape: ', train_labels[0].shape)\n",
    "print('test_set: ', np.sum(ts_msk))\n",
    "print('train_set: ',np.sum(tr_msk))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: divide by zero encountered in power\n"
     ]
    }
   ],
   "source": [
    "train_f=[]\n",
    "train_a=[]\n",
    "for i in range(20):\n",
    "    train_f.append(preprocess_features(train_feat[i]))\n",
    "    train_a.append(preprocess_adj_bias(train_adj[i]))\n",
    "    \n",
    "test_f=[]\n",
    "test_a=[]  \n",
    "for i in range(2):\n",
    "    test_f.append(preprocess_features(test_feat[i]))\n",
    "    test_a.append(preprocess_adj_bias(test_adj[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-a26afbca27b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mavg_cost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj_for_ae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mtrain_a\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_msk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_adj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtest_acc1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mts_msk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_a\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model=GAT_Sparse_Inductive([256, 256], [4,4,6], 3480, 50, 121, 5e-3)\n",
    "best=0\n",
    "for epoch in range(1000):\n",
    "    for i in range(20):\n",
    "        avg_cost=0\n",
    "        batch_xs, batch_ys, adj, mask, adj_for_ae = train_f[i*1:(i+1)*1], train_labels[i*1:(i+1)*1],  train_a[i*1:(i+1)*1], tr_msk[i*1:(i+1)*1], train_adj[i*1:(i+1)*1]\n",
    "        model.train(batch_xs[0], batch_ys[0], mask[0][:,np.newaxis], adj[0])\n",
    "    if epoch%50==0:\n",
    "        test_acc1=model.accuracy(test_f[0], test_labels[0], ts_msk[0][:,np.newaxis], test_a[0])\n",
    "        test_acc2=model.accuracy(test_f[1], test_labels[1], ts_msk[1][:,np.newaxis], test_a[1])\n",
    "        print((test_acc1+test_acc2)/2) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
